# Adam Optimizer

> Adam Optimizer is a more advanced variant of Stochastic Gradient Descent (SGD)

Backword Propagation:

> ğŸ’¡ å½“å‰æ—¶åˆ»çš„é€Ÿåº¦æ˜¯å‰ä¸€æ—¶åˆ»é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„å…±åŒä½œç”¨çš„ç»“æœ

$v_{dW} \leftarrow \beta * v_{dW} + (1-\beta) dW$

$v_{db} \leftarrow \beta * v_{db} + (1-\beta) db$
