### RL specialization offered by Univerisity of Albeta @Coursera

This repository is the record and note of this Coursera Course

This course followed the rl textbook by Sutton. 

-------------------
To begin with:

å°å°çš„åæ§½ðŸ¤«: å…¶å®žæ„Ÿè§‰è¿™ä¸ªè¯¾ç¨‹çš„è´¨é‡ä¸é«˜ï¼Œä¸¤ä¸ªè€å¸ˆè®²çš„å¾ˆå¿«ï¼ŒæŽ¨å¯¼ä¹Ÿæ˜¯ä¸€æ­¥å¸¦è¿‡ã€‚å¥½å‡ ä¸ªvideoéƒ½æ˜¯åå¤çœ‹äº†å¥½å‡ éä¹Ÿæ²¡å¬æ‡‚ï¼Œä½†æ˜¯ä¸€çœ‹ä¹¦å°±å¾ˆæ¸…æ¥šã€‚
è¯´å®žè¯æˆ‘æœ€çƒ¦çš„å°±æ˜¯ç»™æ•°å­¦å…¬å¼è·³æ­¥ï¼Œbellman equationé‡Œé¢å¯¹å“ªä¸ªRandom Variableæ±‚æœŸæœ›éƒ½æ²¡è¯´æ¸…æ¥šï¼Œç›´æŽ¥ç»™æŽ¨å¯¼å…¬å¼ã€‚åˆšå¼€å§‹çœ‹çš„æ—¶å€™ä¸€å¤´é›¾æ°´ã€‚ðŸ˜¡
æˆ‘åœ¨çœ‹è¿™ä¸ªè¯¾çš„åŒæ—¶ä¹Ÿçœ‹äº†èŽ«çƒ¦çš„rlè¯¾å’ŒçŽ‹æ ‘æ£®çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ã€‚çŽ‹æ ‘æ£®çš„æŽ¨å¯¼ç›¸å¯¹æ›´åŠ è¯¦ç»†ï¼ŒèŽ«çƒ¦çš„ä¾‹å­å¾ˆæœ‰æ„æ€ã€‚
æˆ‘ä¸€èˆ¬åœ¨åƒé¥­çš„æ—¶å€™çœ‹çŽ‹æ ‘æ£®çš„è¯¾ï¼Œä¸€è¾¹åƒä¸€è¾¹è®°ç¬”è®°æƒ³æŽ¨å¯¼ã€‚åƒå®Œä¹‹åŽå°±ç›´æŽ¥çœ‹ä»–çš„noteï¼Œå°±å‘çŽ°å¥½åƒéƒ½çœ‹æ‡‚äº†ã€‚åæ­£çœ‹uaçš„rlè¯¾çœ‹ä¸æ‡‚äº†å°±åŽ»çœ‹çŽ‹æ ‘æ£®ðŸ¶

--------------------

Certificate:

1. [Fundamentals of Reinforcement Learning](https://github.com/yoyostudy/rl_ua/blob/main/Certificates/UA_RL_1.pdf)
2. [Sample-based Learning Method](https://github.com/yoyostudy/rl_ua/blob/main/Certificates/UA_RL_2.pdf)
3. [Prediction and Control with Function Approximation](https://github.com/yoyostudy/rl_ua/blob/main/Certificates/UA_RL_3.pdf)
4. [A Complete Reinforcement Learning System](https://github.com/yoyostudy/rl_ua/blob/main/Certificates/UA_RL_4.pdf)

Assignments:

- Course-1-Assignment-1: [Bandit Exploration and Exploitation](https://github.com/yoyostudy/rl_ua/tree/main/code/C1_W1_A1_bandit_exploration_eploitation/Bandits)
- Course-1-Assignment-2: [Optimal Policies with Dynamic Programming](https://github.com/yoyostudy/rl_ua/blob/main/code/C1_W1_A2_GridworldCityParking_DP/DynamicProgramming/Assignment2.ipynb) ðŸ”— Sutton Chapter 4
      
      Key Words: Model-based, Policy Evaluation, Policy Iteration, Value Iteration
- Course-2-Assignments: ðŸ”— Sutton Chapter 6
  
  [Policy Evaluation in cliff Walking Environment](https://github.com/yoyostudy/rl_ua/tree/main/code/C2_A1_CliffWalking_PolicyEvaluation/Policy%20Evaluation%20with%20Temporal%20Difference%20Learning)
  
  [Q-learning and Expected SARSA in cliff Walking Environment](https://github.com/yoyostudy/rl_ua/tree/main/code/C2_A2_Qlearning_SARSA_CliffWalking/Q-Learning%20and%20Expected%20Sarsa)

      key Words: TD, Model-free, boostrapping, TD(0), policy-based
      
  
## Induction on Bellman Equation 

> Bellman equation for value function. It expresses a relationship between the value of a state and the values of its successor states

$$v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s',r|s,a) (r + \gamma * v_{\pi}(s')) = E_{\pi} [R_t + \gamma* v_{\pi}(S_{t+1}) | S_t = s]$$

Proof:
- $v_{\pi}(s) := E_{\pi}(U_t|S_t = s) = E_{\pi}(R_t + \gamma * U_{t+1}) |S_t = s) = E_{\pi}(R_t|S_t = s) + \gamma * E_{\pi}(U_{t+1}|S_t = s)$
- $E_{\pi}(R_t|S_t = s)  = \sum_{r} r \cdot p(r|s) = \sum_r r \cdot \sum_a p(r,a|s) = \sum_r \sum_a r p(r|a,s) \pi(a|s) = \sum_a \pi(a|s) \sum_r \sum_{s'} r p(s',r|s,a) = \sum_a \pi(a|s) \sum_{r, s'} p(s',r|s,a) \cdot r$
- $E_{\pi}(U_{t+1}|S_t = s) = \sum_{u_{t+1}} u_{t+1} p(u_{t+1} |s) = \sum_{u_{t+1}} u_{t+1} \sum_a  p(u_{t+1}, a|s) =  \sum_{u_{t+1}} u_{t+1} \sum_a  p(u_{t+1}|s, a) \pi(a|s) = \sum_a \pi(a|s)  \sum_{u_{t+1}} u_{t+1} \cdot p(u_{t+1}|s, a)$
  $= \sum_a \pi(a|s) \sum_{u_{t+1}} u_{t+1} \sum_{s'} p(u_{t+1}, s'|s, a)$
  $= \sum_a \pi(a|s) \sum_{u_{t+1}} u_{t+1} \sum_{s'} p(u_{t+1}| s', s, a)p(s'|s,a) $
  $= \sum_a \pi(a|s) \sum_{u_{t+1}} u_{t+1} \sum_{s'} p(u_{t+1}| s') \sum_r p(s', r|s,a) $
  $= \sum_a \pi(a|s) \sum_{s'} \sum_r  p(s',r |s,a) \sum_{u_{t+1}}  p(u_{t+1}| s') \cdot u_{t+1} $
  $= \sum_a \pi(a|s) \sum_{s'} \sum_r  p(s',r |s,a) E[U_{t+1} | s']$
  $= \sum_a \pi(a|s) \sum_{s',r } p(s',r |s,a) v_{\pi}(s')$
- Write in the form of expectation, note that all the upper case letter are random variables where the expectations are performed.




$$q_{\pi}(s,a) = \sum_{s',r} p(r,s'|s,a) [r + \gamma * \sum_{a'} \pi(a'|s') q_{\pi}(s',a')]$$

> Connect $V_{\pi}(s)$ with $Q_{\pi}(s,a)$ 

$$v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(a,s)$$

å›¾ç‰‡æ­£åœ¨åŠ è½½ä¸­....

> Connect $Q_{\pi}(s,a)$ with $V_{\pi}(s')$
  
  
  










