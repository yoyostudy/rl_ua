# Adam Optimizer

> Adam Optimizer is a more advanced variant of Stochastic Gradient Descent (SGD)

Backword Propagation:

> 💡 当前时刻的速度是前一时刻速度和加速度的共同作用的结果

$v_{dW} \leftarrow \beta * v_{dW} + (1-\beta) dW$

$v_{db} \leftarrow \beta * v_{db} + (1-\beta) db$
